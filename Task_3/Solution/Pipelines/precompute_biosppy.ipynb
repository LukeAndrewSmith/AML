{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# ML\n",
    "\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import VarianceThreshold, mutual_info_classif, SelectFromModel\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# feature extraction\n",
    "import biosppy.signals.ecg as ecg\n",
    "from tsfresh.utilities.dataframe_functions import impute\n",
    "from tsfresh import feature_extraction as tsfe\n",
    "from tsfresh import extract_relevant_features\n",
    "\n",
    "\n",
    "# Custom\n",
    "import sys,os\n",
    "sys.path.append( '.' )\n",
    "sys.path.append( '..' )\n",
    "import Components.Outlier_Detection as Outlier_Detection\n",
    "import Components.Feature_Selection as Feature_Selection\n",
    "import Components.Normalisation as Normalisation\n",
    "import Components.data_fetching as data_fetching\n",
    "import Components.feature_extraction as feature_extraction\n",
    "\n",
    "# CAREFUL:\n",
    "# If you make changes to a custom module, you have to reload it, i.e rerun this cell\n",
    "import importlib\n",
    "importlib.reload(Outlier_Detection)\n",
    "importlib.reload(Feature_Selection)\n",
    "importlib.reload(Normalisation)\n",
    "importlib.reload(data_fetching)\n",
    "importlib.reload(feature_extraction)\n",
    "\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = data_fetching.get_train_data()\n",
    "x_test = data_fetching.get_test_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_new = []\n",
    "for index, row in X.iterrows(): \n",
    "    _, _, peaks, _, templates, _, _ = ecg.ecg(signal=row.dropna(), sampling_rate=300.0, show=False)\n",
    "    #get one averaged heartbeat template for each time series\n",
    "    average = np.mean(templates, axis=0)\n",
    "    #calculate the variances of the heartbeat templates for a selected number of points (evenly distributed)\n",
    "    sel_templates = templates[np.round(np.linspace(0, len(templates)-1, 20)).astype(int)]\n",
    "    variances = np.var(sel_templates,axis=0)\n",
    "    #calculate the distances between r-peaks\n",
    "    peaks_distances = np.diff(peaks)\n",
    "    mean_peaks_distances = np.mean(peaks_distances)\n",
    "    var_peaks_distances = np.var(peaks_distances)\n",
    "    features = np.concatenate([average,variances,[mean_peaks_distances,var_peaks_distances]])\n",
    "    X_train_new.append(features)\n",
    "X_train_new = pd.DataFrame(X_train_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "store the selected features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_new.to_csv('../../Data/heartbeat_feat_train.csv', index_label='id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "calculate the same features for the test data and store it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_new = []\n",
    "for index, row in x_test.iterrows(): \n",
    "    _, _, peaks, _, templates, _, _ = ecg.ecg(signal=row.dropna(), sampling_rate=300.0, show=False)\n",
    "    #get one averaged heartbeat template for each time series\n",
    "    average = np.mean(templates, axis=0)\n",
    "    #calculate the variances of the heartbeat templates for a selected number of points (evenly distributed)\n",
    "    sel_templates = templates[np.round(np.linspace(0, len(templates)-1, 20)).astype(int)]\n",
    "    variances = np.var(sel_templates,axis=0)\n",
    "    #calculate the distances between r-peaks\n",
    "    peaks_distances = np.diff(peaks)\n",
    "    mean_peaks_distances = np.mean(peaks_distances)\n",
    "    var_peaks_distances = np.var(peaks_distances)\n",
    "    features = np.concatenate([average,variances,[mean_peaks_distances,var_peaks_distances]])\n",
    "    X_test_new.append(features)\n",
    "X_test_new = pd.DataFrame(X_test_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_new.to_csv('../../Data/heartbeat_feat_test.csv', index_label='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cpu.m58",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m58"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
