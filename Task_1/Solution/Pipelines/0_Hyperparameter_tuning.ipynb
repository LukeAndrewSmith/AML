{"cells":[{"cell_type":"markdown","source":"# Hyperparameter Tuning Pipeline\n### This pipeline is used to specify all possible hp and tune them","metadata":{"tags":[],"output_cleared":false,"cell_id":"00000-b2b9a34f-08db-4d8d-86ef-8058877a152e"}},{"cell_type":"markdown","source":"## Imports","metadata":{"tags":[],"output_cleared":false,"cell_id":"00001-7a20eb7b-3f40-4709-8fc2-d306667ba405"}},{"cell_type":"code","source":"!pip install xgboost","metadata":{"tags":[],"cell_id":"00002-e7f47870-64be-4ea5-a3d8-dfcbadbee9c8","output_cleared":false,"source_hash":"4d27a39e","execution_start":1602752563985,"execution_millis":2448},"outputs":[{"name":"stdout","text":"Requirement already satisfied: xgboost in /opt/venv/lib/python3.7/site-packages (1.2.1)\nRequirement already satisfied: scipy in /opt/venv/lib/python3.7/site-packages (from xgboost) (1.5.2)\nRequirement already satisfied: numpy in /opt/venv/lib/python3.7/site-packages (from xgboost) (1.19.2)\n\u001b[33mWARNING: You are using pip version 20.2.2; however, version 20.2.3 is available.\nYou should consider upgrading via the '/opt/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","metadata":{"id":"_1DeRFrUzVGn","output_cleared":false,"source_hash":"85b8064","execution_millis":5,"cell_id":"00002-624f4e06-512d-411d-b2be-d9d703d700bd","execution_start":1602752786717},"source":"# General\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\n\n# ML\nfrom sklearn.linear_model import LinearRegression, Lasso, BayesianRidge, LassoLarsCV, LassoCV, Ridge\nfrom sklearn.metrics import r2_score\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.feature_selection import RFE\nimport xgboost as xgb\n#from sklearn.preprocessing import StandardScaler\n\n# Custom\nimport sys,os\n#%cd /content/gdrive/My Drive/ETHZ/Autumn2020/AML/Git/AML/Task_1/Solution\n#sys.path.append('/content/gdrive/My Drive/ETHZ/Autumn2020/AML/Git/AML/Task_1/Solution')\nsys.path.append('/home/jovyan/work/AML/Task_1/Solution') # I would like a cleaner solution but works for now\nimport Components.Imputation as Imputation\nimport Components.Outlier_Detection_1D as Outlier_Detection_1D\nimport Components.Outlier_Detection_ND as Outlier_Detection_ND\nimport Components.Feature_Selection as Feature_Selection\nimport Components.Normalisation as Normalisation\nimport Components.data_fetching as data_fetching\n\n# CAREFUL:\n# If you make changes to a custom module, you have to reload it, i.e rerun this cell\nimport importlib\nimportlib.reload(Imputation)\nimportlib.reload(Outlier_Detection_1D)\nimportlib.reload(Outlier_Detection_ND)\nimportlib.reload(Feature_Selection)\nimportlib.reload(Normalisation)\nimportlib.reload(data_fetching)","execution_count":10,"outputs":[{"output_type":"execute_result","execution_count":10,"data":{"text/plain":"<module 'Components.data_fetching' from '/home/jovyan/work/AML/Task_1/Solution/Components/data_fetching.py'>"},"metadata":{}}]},{"cell_type":"markdown","source":"## Data Cleaning","metadata":{"tags":[],"output_cleared":false,"cell_id":"00003-3f63f208-142f-426a-9dd8-31b675ef1bca"}},{"cell_type":"markdown","source":"### Data import","metadata":{"tags":[],"output_cleared":false,"cell_id":"00004-30bc22a2-04a9-4f08-bac9-9136e09acdbc"}},{"cell_type":"code","metadata":{"tags":[],"output_cleared":false,"source_hash":"e7420133","execution_millis":1253,"cell_id":"00005-221449e5-cf33-4de4-980e-b2858c007cdc","execution_start":1602752583464},"source":"x_train, y_train = data_fetching.get_train_data()\nx_test = data_fetching.get_test_data()","execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"### First (simple) imputation","metadata":{"tags":[],"output_cleared":false,"cell_id":"00006-a5b2be93-acc7-4eb1-846d-7f715e7a4802"}},{"cell_type":"code","metadata":{"tags":[],"output_cleared":false,"source_hash":"a0bb1ede","execution_millis":556,"cell_id":"00007-61bffffd-1cc5-4477-b73c-b2523ec85b73","execution_start":1602752586127},"source":"print(\"Missing values before simple median imputation: \")\nprint(x_train.isnull().sum().sum())\nmissing_values = Imputation.missing_values_mask(x_train)\n\nx_train = Imputation.median(x_train, x_test)\n\nprint(\"Missing values after simple median imputation: \")\nprint(x_train.isnull().sum().sum())","execution_count":4,"outputs":[{"name":"stdout","text":"Missing values before simple median imputation: \n76910\nMissing values after simple median imputation: \n0\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## BIG LOOP","metadata":{"tags":[],"cell_id":"00010-6bd200d9-f007-4cf2-b382-4a0f0ead5c65","output_cleared":false}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00011-6610cea0-1f05-4183-ab5c-266e288e59c3","output_cleared":false,"source_hash":"34335b08","execution_millis":19,"execution_start":1602752817144},"source":"normalization_algos = [Normalisation.gaussian] #, 'to implement: yeo-johnson']\none_dim_outlier_algos = [Outlier_Detection_1D.z_score]\nmagic_indices_outlier_numbers = [i for i in range(0,100,25)]\nimpute_algos = [Imputation.mean2, Imputation.iterative_regression2]\nfeature_selections = ['LassoLarsCV', 'RFE'] #, ' to implement correlation']\nmodels = ['xgbr', 'GradientBoostingRegressor', 'LassoCV'] #, 'todo what other models to test?']","execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00012-1c605b51-01d2-408d-a0a9-25679c4980b8","output_cleared":false,"source_hash":"fe5fb18e","execution_millis":355294,"execution_start":1602754202574},"source":"all_scores = list()\nall_scores_mean = list()\nconfig = list()\nimport json\n# need to rename the x train to get a consistent loop\nfor norm in normalization_algos:\n    ### normalization ###\n    print('normalisation')\n    print(norm)\n    x_train_norm = norm(x_train)\n    x_test_norm = norm(x_test)\n    for one_dim_outlier_alg in one_dim_outlier_algos:\n        ### 1d outlier ###\n        print('1d outlier')\n        print(one_dim_outlier_alg)\n        x_train_1d = one_dim_outlier_alg(x_train_norm)\n        for magic_indices_outlier_number in magic_indices_outlier_numbers:\n            ### md outlier ###\n            print('md outlier n outliers')\n            print(magic_indices_outlier_number)\n            x_train_md, y_train_md, missing_values_md = Outlier_Detection_ND.magic_indices(x_train_1d,y_train,n_outliers=magic_indices_outlier_number, mask=missing_values)\n            for impute_alg in impute_algos:\n                ### imputation ###\n                print('imputation')\n                print(impute_alg)\n                x_train_impute = x_train_md.mask(missing_values_md)\n                x_train_impute, x_test_impute = impute_alg(x_train_impute,x_test_norm)\n                for feature_selection in feature_selections:\n                    for model in models:\n                        print('pipeline')\n                        print(feature_selection)\n                        print(model)\n                        ### pipeline ###\n                        # maybe some ifs needed to configure different models here:\n                        if feature_selection == 'LassoLarsCV':\n                            if model == 'GradientBoostingRegressor':\n                                pipe = Pipeline([('feature_selection', SelectFromModel(LassoLarsCV())),\n                                ('regressor', GradientBoostingRegressor())])\n                            elif model == 'LassoCV':\n                                # dont know if alpha needs more fitting\n                                pipe = Pipeline([('feature_selection', SelectFromModel(LassoLarsCV())),\n                                ('regressor', LassoCV(random_state=0))])\n                            elif model == 'xgbr':\n                                pipe = Pipeline([('feature_selection', SelectFromModel(LassoLarsCV())),\n                                ('regressor', xgb.XGBRegressor(max_depth=3, n_estimators=75, learning_rate=0.05, alpha=1, objective='reg:squarederror'))])\n\n                        elif feature_selection == 'RFE':\n                            if model == 'GradientBoostingRegressor':\n                                pipe = Pipeline([('feature_selection', SelectFromModel(RFE(Ridge(), n_features_to_select=50()))),\n                                ('regressor', GradientBoostingRegressor())])\n                            elif model == 'LassoCV':\n                                # dont know if alpha needs more fitting\n                                pipe = Pipeline([('feature_selection', SelectFromModel(RFE(Ridge(), n_features_to_select=50()))),\n                                ('regressor', LassoCV(random_state=0))])\n                            elif model == 'xgbr':\n                                pipe = Pipeline([('feature_selection', SelectFromModel(RFE(Ridge(), n_features_to_select=50()))),\n                                ('regressor', xgb.XGBRegressor(max_depth=3, n_estimators=75, learning_rate=0.05, alpha=1, objective='reg:squarederror'))])\n                        \n                        scores = cross_val_score(pipe, x_train_impute, np.ravel(y_train_md), cv=5, scoring='r2')\n                        mean = np.mean(scores)\n                        print(scores)\n                        print(mean)\n                        all_scores.append(scores)\n                        all_scores_mean.append(mean)\n                        loop_config = {\n                            'normalization' : str(norm),\n                            'one_dim_outlier' : str(one_dim_outlier_alg),\n                            'magic_indices_n_outlier' : magic_indices_outlier_number,\n                            'imputation' : str(impute_alg),\n                            'feature_selection' : str(feature_selection),\n                            'model' : str(model),\n                            'scores' : list(scores),\n                            'mean_score' : mean\n                        }\n                        config.append(loop_config.copy())\n                        with open('../../Predictions/hyperparameter_tuning.json', 'w') as fout:\n                            fout.write(json.dumps(config))\n\n\n","execution_count":13,"outputs":[{"name":"stdout","text":"normalisation\n<function gaussian at 0x7fdfb29de268>\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Final Prediction","metadata":{"tags":[],"output_cleared":false,"cell_id":"00021-d8a4a819-0703-4ddf-a06a-710ba78102da"}},{"cell_type":"code","metadata":{"tags":[],"output_cleared":false,"source_hash":"5986ea67","execution_millis":174,"cell_id":"00023-bead5f24-2052-4d21-a194-f74ddea4d028"},"source":"pipe.fit(x_train, np.ravel(y_train))\n\n# Note: They said no outliers were introduced in x_test so no need to perform outlier detection\ny_pred = pipe.predict(x_test)\nplt.hist(y_pred)\n\ny_pred_pd = pd.DataFrame(data=y_pred, columns=[\"y\"])\ny_pred_pd.to_csv('../../Predictions/TODO_Give_new_name.csv', index_label='id')","execution_count":null,"outputs":[]}],"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"task1.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"deepnote_notebook_id":"5f3fe337-073d-4180-8a53-5824826dd43e","deepnote_execution_queue":[{"cellId":"00012-1c605b51-01d2-408d-a0a9-25679c4980b8","sessionId":"666a5a25-c99f-4752-92b2-6fb6b7fcfdbd","msgId":"9b15c408-31f9-4765-b51e-2764f77852ac"}]}}