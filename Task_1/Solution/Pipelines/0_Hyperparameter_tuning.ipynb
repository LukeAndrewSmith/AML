{"cells":[{"cell_type":"markdown","source":"# Hyperparameter Tuning Pipeline\n### This pipeline is used to specify all possible hp and tune them","metadata":{"tags":[],"output_cleared":false,"cell_id":"00000-b2b9a34f-08db-4d8d-86ef-8058877a152e"}},{"cell_type":"markdown","source":"## Imports","metadata":{"tags":[],"output_cleared":false,"cell_id":"00001-7a20eb7b-3f40-4709-8fc2-d306667ba405"}},{"cell_type":"code","metadata":{"id":"_1DeRFrUzVGn","output_cleared":false,"source_hash":"e75de7bf","execution_millis":32,"cell_id":"00002-624f4e06-512d-411d-b2be-d9d703d700bd","execution_start":1602665431498},"source":"# General\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\n\n# ML\nfrom sklearn.linear_model import LinearRegression, Lasso, BayesianRidge, LassoLarsCV, LassoCV, Ridge\nfrom sklearn.metrics import r2_score\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.feature_selection import RFE\n\n#from sklearn.preprocessing import StandardScaler\n\n# Custom\nimport sys,os\nsys.path.append('/home/jovyan/work/AML/Task_1/Solution') # I would like a cleaner solution but works for now\nimport Components.Imputation as Imputation\nimport Components.Outlier_Detection_1D as Outlier_Detection_1D\nimport Components.Outlier_Detection_ND as Outlier_Detection_ND\nimport Components.Feature_Selection as Feature_Selection\nimport Components.Normalisation as Normalisation\nimport Components.data_fetching as data_fetching\n\n# CAREFUL:\n# If you make changes to a custom module, you have to reload it, i.e rerun this cell\nimport importlib\nimportlib.reload(Imputation)\nimportlib.reload(Outlier_Detection_1D)\nimportlib.reload(Outlier_Detection_ND)\nimportlib.reload(Feature_Selection)\nimportlib.reload(Normalisation)\nimportlib.reload(data_fetching)","execution_count":null,"outputs":[{"output_type":"execute_result","execution_count":41,"data":{"text/plain":"<module 'Components.data_fetching' from '/home/jovyan/work/AML/Task_1/Solution/Components/data_fetching.py'>"},"metadata":{}}]},{"cell_type":"markdown","source":"## Data Cleaning","metadata":{"tags":[],"output_cleared":false,"cell_id":"00003-3f63f208-142f-426a-9dd8-31b675ef1bca"}},{"cell_type":"markdown","source":"### Data import","metadata":{"tags":[],"output_cleared":false,"cell_id":"00004-30bc22a2-04a9-4f08-bac9-9136e09acdbc"}},{"cell_type":"code","metadata":{"tags":[],"output_cleared":false,"source_hash":"5446b28","execution_millis":3055,"cell_id":"00005-221449e5-cf33-4de4-980e-b2858c007cdc","execution_start":1602665434111},"source":"x_train, y_train = data_fetching.get_train_data()\nx_test = data_fetching.get_test_data()","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### First (simple) imputation","metadata":{"tags":[],"output_cleared":false,"cell_id":"00006-a5b2be93-acc7-4eb1-846d-7f715e7a4802"}},{"cell_type":"code","metadata":{"tags":[],"output_cleared":false,"source_hash":"a0bb1ede","execution_millis":1172,"cell_id":"00007-61bffffd-1cc5-4477-b73c-b2523ec85b73","execution_start":1602665438775},"source":"print(\"Missing values before simple median imputation: \")\nprint(x_train.isnull().sum().sum())\nmissing_values = Imputation.missing_values_mask(x_train)\n\nx_train = Imputation.median(x_train, x_test)\n\nprint(\"Missing values after simple median imputation: \")\nprint(x_train.isnull().sum().sum())","execution_count":null,"outputs":[{"name":"stdout","text":"Missing values before simple median imputation: \n76910\nMissing values after simple median imputation: \n0\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## BIG LOOP","metadata":{"tags":[],"cell_id":"00010-6bd200d9-f007-4cf2-b382-4a0f0ead5c65","output_cleared":false}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00011-6610cea0-1f05-4183-ab5c-266e288e59c3","output_cleared":false,"source_hash":"6c058775","execution_millis":31,"execution_start":1602665441226},"source":"normalization_algos = [Normalisation.gaussian] #, 'to implement: yeo-johnson']\none_dim_outlier_algos = [Outlier_Detection_1D.z_score]\nmagic_indices_outlier_numbers = [i for i in range(0,100,20)]\nimpute_algos = [Imputation.knn2, Imputation.iterative_regression2]\nfeature_selections = ['LassoLarsCV', 'RFE'] #, ' to implement correlation']\nmodels = ['xgbr', 'GradientBoostingRegressor', 'LassoCV'] #, 'todo what other models to test?']","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00012-1c605b51-01d2-408d-a0a9-25679c4980b8","output_cleared":false,"source_hash":"7ed16db9","execution_millis":942845,"execution_start":1602665448551},"source":"all_scores = list()\nall_scores_mean = list()\nconfig = list()\n# need to rename the x train to get a consistent loop\nfor norm in normalization_algos:\n    ### normalization ###\n    print('normalisation')\n    print(norm)\n    x_train_norm = norm(x_train)\n    x_test_norm = norm(x_test)\n    for one_dim_outlier_alg in one_dim_outlier_algos:\n        ### 1d outlier ###\n        print('1d outlier')\n        print(one_dim_outlier_alg)\n        x_train_1d = one_dim_outlier_alg(x_train_norm)\n        for magic_indices_outlier_number in magic_indices_outlier_numbers:\n            ### md outlier ###\n            print('md outlier n outliers')\n            print(magic_indices_outlier_number)\n            x_train_md, y_train_md, missing_values_md = Outlier_Detection_ND.magic_indices(x_train_1d,y_train,n_outliers=magic_indices_outlier_number, mask=missing_values)\n            for impute_alg in impute_algos:\n                ### imputation ###\n                print('imputation')\n                print(impute_alg)\n                x_train_impute = x_train_md.mask(missing_values_md)\n                x_train_impute, x_test_impute = impute_alg(x_train_impute,x_test_norm)\n                for feature_selection in feature_selections:\n                    for model in models:\n                        print('pipeline')\n                        print(feature_selection)\n                        print(model)\n                        ### pipeline ###\n                        # maybe some ifs needed to configure different models here:\n                        if feature_selection == 'LassoLarsCV':\n                            if model == 'GradientBoostingRegressor':\n                                pipe = Pipeline([('feature_selection', SelectFromModel(LassoLarsCV()),\n                        ('regressor', GradientBoostingRegressor()])\n                            elif model == 'LassoCV':\n                                # dont know if alpha needs more fitting\n                                pipe = Pipeline([('feature_selection', SelectFromModel(LassoLarsCV()),\n                        ('regressor', LassoCV(random_state=0)])\n                            elif model == 'xgbr':\n                                pipe = Pipeline([('feature_selection', SelectFromModel(LassoLarsCV()),\n                        ('regressor', xgb.XGBRegressor(max_depth=3, n_estimators=75, learning_rate=0.05, alpha=1, objective='reg:squarederror')])\n\n                        elif feature_selection == 'RFE':\n                            if model == 'GradientBoostingRegressor':\n                                pipe = Pipeline([('feature_selection', SelectFromModel(RFE(Ridge(), n_features_to_select=50()),\n                        ('regressor', GradientBoostingRegressor()])\n                            elif model == 'LassoCV':\n                                # dont know if alpha needs more fitting\n                                pipe = Pipeline([('feature_selection', SelectFromModel(RFE(Ridge(), n_features_to_select=50()),\n                        ('regressor', LassoCV(random_state=0)])\n                            elif model == 'xgbr':\n                                pipe = Pipeline([('feature_selection', SelectFromModel(RFE(Ridge(), n_features_to_select=50()),\n                        ('regressor', xgb.XGBRegressor(max_depth=3, n_estimators=75, learning_rate=0.05, alpha=1, objective='reg:squarederror')])\n                        \n                        scores = cross_val_score(pipe, x_train_impute, np.ravel(y_train_md), cv=5, scoring='r2')\n                        mean = np.mean(scores)\n                        print(scores)\n                        print(mean)\n                        all_scores.append(scores)\n                        all_scores_mean.append(mean)\n                        loop_config = {\n                            'normalization' : str(norm),\n                            'one_dim_outlier' : str(one_dim_outlier_alg),\n                            'magic_indices_n_outlier' : magic_indices_outlier_number,\n                            'imputation' : str(impute_alg),\n                            'feature_selection' : str(feature_selection),\n                            'model' : str(model),\n                            'scores' : scores,\n                            'mean_score' : mean\n                        }\n                        config.append(loop_config.copy())\n                        \n                        \n\n# save config to file\nimport json\nwith open('../../Predictions/hyperparameter_tuning.json', 'w') as fout:\n    fout.write(json.dumps(config))\n","execution_count":null,"outputs":[{"name":"stdout","text":"normalisation\n<function gaussian at 0x7f7163634bf8>\n1d outlier\n<function z_score at 0x7f715d360950>\nmd outlier n outliers\n10\nimputation\n<function knn2 at 0x7f715d360a60>\npipeline\n<class 'sklearn.linear_model._least_angle.LassoLarsCV'>\n<class 'sklearn.ensemble._gb.GradientBoostingRegressor'>\n[0.62984637 0.56484081 0.51662229 0.52947056 0.61813774]\n0.5717835561408494\npipeline\n<class 'sklearn.linear_model._least_angle.LassoLarsCV'>\n<class 'sklearn.linear_model._coordinate_descent.Lasso'>\n","output_type":"stream"},{"name":"stdout","text":"[0.52484922 0.49012091 0.40378024 0.48554683 0.53890757]\n0.4886409504382453\nimputation\n<function iterative_regression2 at 0x7f7163634400>\n","output_type":"stream"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-45-35f3cd0a042b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimpute_alg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m                 \u001b[0mx_train_impute\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_train_md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_values_md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                 \u001b[0mx_train_impute\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test_impute\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimpute_alg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train_impute\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_test_norm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mfeature_selection\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeature_selections\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/work/AML/Task_1/Solution/Components/Imputation.py\u001b[0m in \u001b[0;36miterative_regression2\u001b[0;34m(X, x_supp, n_nearest_features)\u001b[0m\n\u001b[1;32m     60\u001b[0m     imp = IterativeImputer(missing_values=np.nan, \n\u001b[1;32m     61\u001b[0m                             max_iter=10, initial_strategy='median',random_state=0, n_nearest_features=n_nearest_features)\n\u001b[0;32m---> 62\u001b[0;31m     \u001b[0mimp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_supp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_supp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/venv/lib/python3.7/site-packages/sklearn/impute/_iterative.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    738\u001b[0m             \u001b[0mReturns\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m         \"\"\"\n\u001b[0;32m--> 740\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    741\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/venv/lib/python3.7/site-packages/sklearn/impute/_iterative.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    641\u001b[0m                 Xt, estimator = self._impute_one_feature(\n\u001b[1;32m    642\u001b[0m                     \u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_missing_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeat_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneighbor_feat_idx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 643\u001b[0;31m                     estimator=None, fit_mode=True)\n\u001b[0m\u001b[1;32m    644\u001b[0m                 estimator_triplet = _ImputerTriplet(feat_idx,\n\u001b[1;32m    645\u001b[0m                                                     \u001b[0mneighbor_feat_idx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/venv/lib/python3.7/site-packages/sklearn/impute/_iterative.py\u001b[0m in \u001b[0;36m_impute_one_feature\u001b[0;34m(self, X_filled, mask_missing_values, feat_idx, neighbor_feat_idx, estimator, fit_mode)\u001b[0m\n\u001b[1;32m    301\u001b[0m             y_train = _safe_indexing(X_filled[:, feat_idx],\n\u001b[1;32m    302\u001b[0m                                      ~missing_row_mask)\n\u001b[0;32m--> 303\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    304\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m         \u001b[0;31m# if no missing values, don't predict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/venv/lib/python3.7/site-packages/sklearn/linear_model/_bayes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0mXT_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m         \u001b[0mU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msvd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_matrices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m         \u001b[0meigen_vals_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mS\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/venv/lib/python3.7/site-packages/scipy/linalg/decomp_svd.py\u001b[0m in \u001b[0;36msvd\u001b[0;34m(a, full_matrices, compute_uv, overwrite_a, check_finite, lapack_driver)\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;31m# perform decomposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m     u, s, v, info = gesXd(a1, compute_uv=compute_uv, lwork=lwork,\n\u001b[0;32m--> 126\u001b[0;31m                           full_matrices=full_matrices, overwrite_a=overwrite_a)\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","source":"## Final Prediction","metadata":{"tags":[],"output_cleared":false,"cell_id":"00021-d8a4a819-0703-4ddf-a06a-710ba78102da"}},{"cell_type":"code","metadata":{"tags":[],"output_cleared":false,"source_hash":"5986ea67","execution_millis":174,"cell_id":"00023-bead5f24-2052-4d21-a194-f74ddea4d028"},"source":"pipe.fit(x_train, np.ravel(y_train))\n\n# Note: They said no outliers were introduced in x_test so no need to perform outlier detection\ny_pred = pipe.predict(x_test)\nplt.hist(y_pred)\n\ny_pred_pd = pd.DataFrame(data=y_pred, columns=[\"y\"])\ny_pred_pd.to_csv('../../Predictions/TODO_Give_new_name.csv', index_label='id')","execution_count":null,"outputs":[]}],"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"task1.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"deepnote_notebook_id":"5f3fe337-073d-4180-8a53-5824826dd43e","deepnote_execution_queue":[]}}