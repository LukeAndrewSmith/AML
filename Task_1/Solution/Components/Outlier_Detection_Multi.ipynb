{"cells":[{"cell_type":"code","metadata":{"tags":[],"cell_id":"00000-de4c930f-d6e1-4533-89bd-f8d148ef58eb","output_cleared":false,"source_hash":"fc76d0b8","execution_millis":4021,"execution_start":1602574088442},"source":"# Insert code here.\n# some first basic read ins to exoeriment\n!pip install seaborn","outputs":[{"name":"stdout","text":"Collecting seaborn\n  Downloading seaborn-0.11.0-py3-none-any.whl (283 kB)\n\u001b[K     |████████████████████████████████| 283 kB 5.1 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: matplotlib>=2.2 in /opt/venv/lib/python3.7/site-packages (from seaborn) (3.3.1)\nRequirement already satisfied: pandas>=0.23 in /opt/venv/lib/python3.7/site-packages (from seaborn) (1.0.5)\nRequirement already satisfied: scipy>=1.0 in /opt/venv/lib/python3.7/site-packages (from seaborn) (1.5.2)\nRequirement already satisfied: numpy>=1.15 in /opt/venv/lib/python3.7/site-packages (from seaborn) (1.18.5)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/venv/lib/python3.7/site-packages (from matplotlib>=2.2->seaborn) (1.2.0)\nRequirement already satisfied: python-dateutil>=2.1 in /opt/venv/lib/python3.7/site-packages (from matplotlib>=2.2->seaborn) (2.8.1)\nRequirement already satisfied: certifi>=2020.06.20 in /opt/venv/lib/python3.7/site-packages (from matplotlib>=2.2->seaborn) (2020.6.20)\nRequirement already satisfied: pillow>=6.2.0 in /opt/venv/lib/python3.7/site-packages (from matplotlib>=2.2->seaborn) (7.2.0)\nRequirement already satisfied: cycler>=0.10 in /opt/venv/lib/python3.7/site-packages (from matplotlib>=2.2->seaborn) (0.10.0)\nRequirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /opt/venv/lib/python3.7/site-packages (from matplotlib>=2.2->seaborn) (2.4.7)\nRequirement already satisfied: pytz>=2017.2 in /opt/venv/lib/python3.7/site-packages (from pandas>=0.23->seaborn) (2020.1)\nRequirement already satisfied: six>=1.5 in /opt/venv/lib/python3.7/site-packages (from python-dateutil>=2.1->matplotlib>=2.2->seaborn) (1.15.0)\nInstalling collected packages: seaborn\nSuccessfully installed seaborn-0.11.0\n\u001b[33mWARNING: You are using pip version 20.2.2; however, version 20.2.3 is available.\nYou should consider upgrading via the '/opt/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00001-798b4389-f5f5-46fb-b775-2a56e0d55bb2","output_cleared":false,"source_hash":"d035b9a8","execution_millis":2,"execution_start":1602574222408},"source":"import pandas as pd\nimport numpy as np\nfrom sklearn.impute import SimpleImputer\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import figure\nfrom sklearn.preprocessing import PowerTransformer\nfrom sklearn.covariance import EmpiricalCovariance\nimport seaborn as sb\nfrom sklearn.covariance import EllipticEnvelope","outputs":[],"execution_count":null},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00002-1aaa5f68-31a4-491a-b4bb-41a33fce2de0","output_cleared":false,"source_hash":"78554dbe","execution_millis":1082,"execution_start":1602574101176},"source":"# read in some data\nimport data_fetching as data_fetching\n\nx_train, y_train = data_fetching.get_train_data()\nx_test = data_fetching.get_test_data()\n","outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### imputation","metadata":{"tags":[],"cell_id":"00003-a785278b-1881-4f6a-a202-547efd28d495"}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00003-6f629e93-4365-4be6-8bad-e2193c111e57","output_cleared":false,"source_hash":"b6033ad8","execution_millis":502,"execution_start":1602574111239},"source":"# Fill values to help outlier detection\n# Use median as it\n\n# impute the nans:\nfill_NaN = SimpleImputer(missing_values=np.nan, strategy='median')\nx_train = pd.DataFrame(fill_NaN.fit_transform(x_train))\nx_test = pd.DataFrame(fill_NaN.fit_transform(x_test))\n\n# Sanity Check\nprint(x_train.isnull().sum().sum())","outputs":[{"name":"stdout","text":"0\n","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"### Outlier detection","metadata":{"tags":[],"cell_id":"00005-b558146a-5c86-4e79-972a-be48f2b68dfa"}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00007-62b7957c-4b1f-4eda-89e0-ce3460278375","output_cleared":false,"source_hash":"67d84117","execution_millis":3626,"execution_start":1602574960387},"source":"import Normalisation as Normalisation\nx_train_norm = Normalisation.gaussian(x_train)\ny_train_norm = Normalisation.gaussian(y_train)\nx_test_norm = Normalisation.gaussian(x_test)\ntrain_all = x_train_norm.copy()\ny_col_norm = y_train_norm.copy()\ntrain_all['age'] = y_col_norm.copy()\nnumpy_all = train_all.to_numpy()\nnumpy_test = x_test_norm.to_numpy()\n\nx_all_norm = x_train_norm.copy()\nx_all_norm = x_all_norm.append(x_test_norm)\n","outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Mahalanobis","metadata":{"tags":[],"cell_id":"00007-dcc7d928-e83c-4cfc-911f-341d8cb45cd5"}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00006-4c98b084-12a1-413b-87c5-0cda336008b4","output_cleared":false,"source_hash":"517642e6","execution_millis":1767,"execution_start":1602524765126},"source":"# fit a Minimum Covariance Determinant (MCD) robust estimator to data\n#robust_cov = MinCovDet().fit(train_all)\n\n# compare estimators learnt from the full data set with true parameters\n#emp_cov = train_all.corr()\ncov = EmpiricalCovariance().fit(x_train)\nprint(cov)\nmahal = cov.mahalanobis(x_train)\nprint(np.shape(mahal))\nprint(np.max(mahal))\nprint(np.mean(mahal))\nsb.boxplot(data=mahal)\ni = np.where(np.asarray(mahal) > 20)\nprint(i)\nprint(train_all.iloc[i])\nmahal_mean = np.mean(mahal)\nmahal_std = np.std(mahal)\nprint('std')\nprint(mahal_std)\n# get all id that have higher z score than alpha and remove them\nj = np.where((np.asarray(mahal) - mahal_mean) / mahal_std > 3)\nprint(j)\n","execution_count":null,"outputs":[{"name":"stdout","text":"EmpiricalCovariance()\n(1212,)\n991.5553115210936\n827.9999999999997\n(array([   0,    1,    2, ..., 1209, 1210, 1211]),)\n             0         1         2         3         4         5         6  \\\n0     0.985497 -1.779499  0.569100 -0.648897  1.715416 -0.972251  1.968190   \n1     1.582564  1.321694  0.000000 -1.158888  0.392737  0.866811  0.900300   \n2     1.166622  1.212859 -0.409335 -1.818220 -0.861723 -0.957059 -0.181168   \n3     1.734518  0.000000  0.000000 -1.684720 -0.643748 -0.635144  0.643726   \n4     0.462699 -0.869882  0.950762  0.000000 -0.147749 -1.712462 -0.255731   \n...        ...       ...       ...       ...       ...       ...       ...   \n1207 -0.230195  0.741100 -0.461705  1.032133  1.143390  0.872991  0.932978   \n1208 -0.307570 -1.495682  0.237800 -0.865120 -0.287572  0.000000 -0.138732   \n1209 -0.845183 -1.531200  0.114189  0.844724 -2.367502 -0.603149  0.000000   \n1210  0.000000 -1.023985  1.290641  0.831294 -0.768299 -1.290620 -0.853901   \n1211 -1.060546 -0.584514  0.269800  0.890330 -0.527102  0.859344  0.533073   \n\n             7         8         9  ...       823       824       825  \\\n0     0.000000  1.510975 -0.731503  ...  0.222310 -0.669060  0.370455   \n1    -1.592517  1.053868  0.986630  ...  0.660993 -1.420039  0.193380   \n2     0.372202  0.388053 -0.149745  ... -0.299691 -1.201889  0.673419   \n3     1.790796  1.669899  0.868028  ... -0.959790  1.450089 -0.271717   \n4     1.499183  0.696491 -1.064102  ... -1.047937  1.906891  0.000000   \n...        ...       ...       ...  ...       ...       ...       ...   \n1207  0.000000  0.312285  0.462310  ... -0.908842  1.745773 -0.300797   \n1208  0.450576  1.155401  2.476816  ... -1.554293  1.120219 -0.916961   \n1209  0.000000  0.982395  0.455407  ...  2.227818 -0.942480 -0.652090   \n1210  1.624922 -1.721847  0.225213  ... -1.318600  0.842008  0.835573   \n1211 -0.319932  0.226134 -0.458217  ...  0.204839  0.586381 -0.777060   \n\n           826       827       828       829       830       831       age  \n0     0.928293  0.764715 -1.469038 -0.485944 -1.511017 -0.609993  0.088734  \n1    -0.667874 -0.661157  0.305557 -1.522273 -0.537695  0.242219  0.308666  \n2     0.366900 -2.216111  0.793439 -0.933083 -2.064168 -0.313152 -0.472789  \n3     0.607561  0.344304  0.392014  1.162156  0.000000 -0.725596 -1.366707  \n4     0.000000  0.795663  0.327783 -0.819305  0.297697  0.243289 -0.348756  \n...        ...       ...       ...       ...       ...       ...       ...  \n1207 -0.402535  0.878717  1.001439  1.517273 -0.366678  0.764436 -0.348756  \n1208 -0.420871  1.481129  1.017402  1.073617  0.819188  5.199338 -2.049594  \n1209  0.391561 -0.447666  0.567427 -1.475996  2.036085  0.271101  1.304923  \n1210  0.173300  0.000000  0.730025 -0.173616  0.551317 -2.013133  0.650837  \n1211 -1.497566  0.836631  0.000000 -0.658350  0.000000 -0.259602  1.168949  \n\n[1212 rows x 833 columns]\nstd\n51.49102622715533\n(array([732]),)\n","output_type":"stream"},{"data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"},"output_type":"display_data"}]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00008-270147ba-73b0-4277-ae64-78cf88830a6d","output_cleared":false,"source_hash":"40bacf7d","execution_millis":1308,"execution_start":1602524622285},"source":"# fit a Minimum Covariance Determinant (MCD) robust estimator to data\n#robust_cov = MinCovDet().fit(train_all)\n\n# compare estimators learnt from the full data set with true parameters\n#emp_cov = train_all.corr()\ncov = EmpiricalCovariance().fit(numpy_test)\nprint(cov)\nmahal = cov.mahalanobis(x_test)\nprint(np.shape(mahal))\nprint(np.max(mahal))\nprint(np.mean(mahal))\nsb.boxplot(data=mahal)\ni = np.where(np.asarray(mahal) > 20)\nprint(i)\nprint(train_all.iloc[i])\nmahal_mean = np.mean(mahal)\nmahal_std = np.std(mahal)\nprint('std')\nprint(mahal_std)\n# get all id that have higher z score than alpha and remove them\nj = np.where((np.asarray(mahal) - mahal_mean) / mahal_std > 5)\nprint(j)\n","execution_count":null,"outputs":[{"name":"stdout","text":"EmpiricalCovariance()\n(776,)\n775.0000000000306\n774.9999999999982\n(array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n        13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n        26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n        39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n        52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,\n        65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n        78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,\n        91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,\n       104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116,\n       117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129,\n       130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142,\n       143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n       156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168,\n       169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n       182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194,\n       195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207,\n       208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220,\n       221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233,\n       234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246,\n       247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259,\n       260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272,\n       273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285,\n       286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298,\n       299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311,\n       312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324,\n       325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337,\n       338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350,\n       351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363,\n       364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376,\n       377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389,\n       390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402,\n       403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415,\n       416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428,\n       429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441,\n       442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454,\n       455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467,\n       468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480,\n       481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493,\n       494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506,\n       507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519,\n       520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532,\n       533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545,\n       546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558,\n       559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571,\n       572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584,\n       585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597,\n       598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610,\n       611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623,\n       624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636,\n       637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649,\n       650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662,\n       663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675,\n       676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688,\n       689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 701,\n       702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714,\n       715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727,\n       728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740,\n       741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753,\n       754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766,\n       767, 768, 769, 770, 771, 772, 773, 774, 775]),)\n          0         1         2         3         4         5         6    \\\n0    0.985497 -1.779499  0.569100 -0.648897  1.715416 -0.972251  1.968190   \n1    1.582564  1.321694  0.000000 -1.158888  0.392737  0.866811  0.900300   \n2    1.166622  1.212859 -0.409335 -1.818220 -0.861723 -0.957059 -0.181168   \n3    1.734518  0.000000  0.000000 -1.684720 -0.643748 -0.635144  0.643726   \n4    0.462699 -0.869882  0.950762  0.000000 -0.147749 -1.712462 -0.255731   \n..        ...       ...       ...       ...       ...       ...       ...   \n771 -0.895677  0.318501 -0.312193  0.944228 -1.637580  0.579699  1.920971   \n772 -0.260443  0.370044  1.849089  0.979044 -1.203788  1.162852  0.224259   \n773 -0.463200 -0.624410 -1.744899  0.000000  0.000000  0.541889 -1.517537   \n774 -0.203479 -0.165602 -0.547728 -0.150040  0.107643 -0.224016 -0.514571   \n775  2.167548 -0.226213 -0.519175  0.461840 -1.303258 -2.325781  0.799362   \n\n          7         8         9    ...       822       823       824  \\\n0    0.000000  1.510975 -0.731503  ... -0.762152  0.222310 -0.669060   \n1   -1.592517  1.053868  0.986630  ...  1.182330  0.660993 -1.420039   \n2    0.372202  0.388053 -0.149745  ... -0.974402 -0.299691 -1.201889   \n3    1.790796  1.669899  0.868028  ... -0.168993 -0.959790  1.450089   \n4    1.499183  0.696491 -1.064102  ... -0.557040 -1.047937  1.906891   \n..        ...       ...       ...  ...       ...       ...       ...   \n771 -0.575491  0.648739  0.883696  ...  0.723674  1.119427  0.313017   \n772 -0.692544  0.544479  1.488822  ...  0.831888 -0.107166 -1.083629   \n773 -1.967367  1.480217  0.376070  ...  2.176031  0.908622 -2.123690   \n774  2.492650  0.151301 -0.931513  ...  0.165323 -2.389884  2.501245   \n775  0.779717 -0.584826 -0.979016  ... -2.804798 -0.092735  0.119354   \n\n          825       826       827       828       829       830       831  \n0    0.370455  0.928293  0.764715 -1.469038 -0.485944 -1.511017 -0.609993  \n1    0.193380 -0.667874 -0.661157  0.305557 -1.522273 -0.537695  0.242219  \n2    0.673419  0.366900 -2.216111  0.793439 -0.933083 -2.064168 -0.313152  \n3   -0.271717  0.607561  0.344304  0.392014  1.162156  0.000000 -0.725596  \n4    0.000000  0.000000  0.795663  0.327783 -0.819305  0.297697  0.243289  \n..        ...       ...       ...       ...       ...       ...       ...  \n771 -2.558254 -0.687850 -0.447499 -0.672486 -0.205176  0.747386 -0.905317  \n772  0.398086 -1.771263 -1.947727 -0.671257 -1.405311 -0.482566 -0.264152  \n773 -1.417023  0.459194  0.000000  0.260851  0.436842  1.799611  1.049753  \n774 -0.453566  0.000000  2.143067 -0.210286  0.205046 -0.217134 -0.176311  \n775 -1.642677  0.967120  0.628983  0.950724  0.609370 -0.458001  1.265593  \n\n[776 rows x 832 columns]\nstd\n7.716840396219295e-12\n(array([], dtype=int64),)\n","output_type":"stream"},{"data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"},"output_type":"display_data"}]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00008-a17b0ad4-9a7f-4077-b85e-44a80efa958c","output_cleared":false,"source_hash":"6c0ec363","execution_millis":24,"execution_start":1602518722686},"source":"print(np.shape(x_train))\nx_train = x_train.drop(x_train.index[j])\nprint(np.shape(x_train))\n\n","execution_count":null,"outputs":[{"name":"stdout","text":"(1212, 833)\n(1207, 833)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### DBSCAN","metadata":{"tags":[],"cell_id":"00010-9b82d5c7-aacc-4215-907b-664f36d61297"}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00009-b36d724d-033f-4cdf-84ca-3e67b56470f3","output_cleared":false,"source_hash":"7889dd02","execution_millis":5185,"execution_start":1602523777690},"source":"from sklearn.cluster import DBSCAN\nimport Normalisation as Normalisation\ntrain_all = Normalisation.gaussian(train_all)\nnumpy_test_all = train_all.to_numpy()\nclustering = DBSCAN(eps=35, min_samples=5).fit(numpy_test_all)\nlabels = clustering.labels_\noutliers = np.where(np.asarray(labels)==-1)\nprint(outliers)\nprint(np.shape(outliers))\n","execution_count":null,"outputs":[{"name":"stdout","text":"(array([   5,   20,   26,   28,   43,   50,   52,   56,   63,   64,   69,\n         90,   91,   93,   97,  108,  111,  112,  113,  122,  123,  128,\n        130,  144,  149,  167,  187,  210,  217,  220,  247,  250,  262,\n        266,  270,  284,  298,  302,  304,  305,  308,  316,  334,  351,\n        352,  383,  384,  417,  418,  420,  421,  422,  426,  429,  431,\n        432,  449,  475,  477,  480,  484,  496,  497,  499,  506,  512,\n        526,  537,  554,  557,  558,  562,  568,  574,  576,  597,  616,\n        618,  619,  626,  633,  634,  644,  689,  694,  701,  703,  748,\n        750,  751,  756,  759,  775,  777,  784,  830,  850,  855,  856,\n        875,  878,  885,  889,  893,  902,  929,  934,  938,  954,  962,\n        979,  989,  990,  995, 1016, 1022, 1027, 1028, 1033, 1040, 1044,\n       1049, 1050, 1057, 1058, 1060, 1061, 1077, 1090, 1096, 1097, 1100,\n       1102, 1117, 1120, 1148, 1150, 1158, 1166, 1178, 1211]),)\n(1, 141)\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00010-48f44307-2a1c-4ea8-a4e0-325d7afc3326","output_cleared":false,"source_hash":"712e2836","execution_millis":2732,"execution_start":1602522884182},"source":"\nfrom sklearn.cluster import DBSCAN\nimport Normalisation as Normalisation\ntest_gaussian = Normalisation.gaussian(x_test)\nnumpy_test_gaussian = test_gaussian.to_numpy()\nclustering = DBSCAN(eps=36, min_samples=5).fit(numpy_test_gaussian)\nlabels = clustering.labels_\noutliers = np.where(np.asarray(labels)==-1)\nprint(outliers)\nprint(np.shape(outliers))\n","execution_count":null,"outputs":[{"name":"stdout","text":"(array([  0,   1,   4,   5,   6,   7,   8,   9,  11,  12,  13,  14,  15,\n        16,  17,  18,  19,  21,  24,  26,  28,  29,  32,  33,  34,  36,\n        37,  39,  40,  41,  43,  44,  46,  47,  49,  50,  51,  52,  53,\n        54,  55,  56,  57,  58,  61,  64,  65,  66,  67,  70,  71,  72,\n        73,  75,  76,  79,  81,  82,  83,  84,  85,  86,  87,  88,  89,\n        90,  91,  92,  93,  95,  96,  98,  99, 101, 102, 103, 104, 106,\n       107, 109, 110, 111, 112, 114, 115, 116, 117, 119, 121, 123, 124,\n       125, 126, 127, 128, 130, 132, 135, 136, 137, 138, 139, 140, 141,\n       142, 144, 145, 147, 149, 150, 151, 152, 154, 155, 156, 159, 160,\n       164, 165, 166, 167, 169, 170, 171, 172, 173, 174, 177, 178, 179,\n       180, 183, 184, 185, 186, 187, 188, 189, 190, 192, 193, 194, 197,\n       198, 199, 200, 202, 204, 206, 207, 209, 212, 213, 214, 215, 217,\n       219, 220, 221, 222, 224, 227, 228, 230, 231, 235, 237, 238, 239,\n       240, 241, 242, 243, 244, 245, 246, 247, 248, 250, 253, 254, 256,\n       257, 258, 260, 261, 262, 263, 265, 267, 268, 269, 271, 272, 275,\n       276, 277, 280, 281, 284, 285, 286, 287, 288, 290, 292, 293, 294,\n       296, 298, 300, 301, 302, 303, 304, 305, 306, 308, 309, 310, 311,\n       313, 314, 315, 317, 318, 319, 320, 321, 323, 325, 327, 328, 329,\n       330, 331, 333, 334, 336, 337, 338, 339, 340, 342, 343, 345, 347,\n       348, 351, 352, 354, 356, 357, 358, 359, 360, 362, 363, 366, 367,\n       368, 369, 374, 378, 380, 381, 382, 383, 386, 387, 388, 389, 390,\n       394, 395, 396, 398, 400, 401, 402, 403, 405, 406, 407, 408, 410,\n       411, 413, 415, 416, 417, 418, 419, 421, 424, 425, 426, 427, 428,\n       429, 430, 432, 433, 434, 435, 436, 437, 438, 439, 441, 446, 447,\n       448, 449, 451, 453, 455, 458, 460, 462, 463, 465, 466, 468, 469,\n       471, 472, 473, 476, 477, 478, 480, 482, 484, 485, 486, 487, 488,\n       489, 491, 493, 494, 495, 496, 497, 498, 500, 501, 503, 504, 505,\n       506, 509, 510, 512, 513, 514, 515, 516, 517, 518, 519, 520, 522,\n       523, 524, 525, 526, 530, 531, 533, 536, 538, 539, 541, 542, 544,\n       546, 547, 548, 549, 550, 551, 553, 554, 555, 556, 558, 560, 561,\n       562, 564, 567, 568, 569, 570, 571, 572, 573, 574, 578, 579, 581,\n       582, 583, 587, 588, 589, 591, 592, 594, 596, 597, 599, 601, 602,\n       603, 604, 605, 606, 607, 608, 609, 610, 613, 615, 617, 620, 621,\n       623, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636,\n       637, 638, 639, 641, 642, 643, 645, 646, 647, 648, 650, 651, 652,\n       653, 655, 656, 658, 660, 662, 663, 664, 665, 667, 669, 670, 671,\n       675, 677, 679, 681, 682, 683, 684, 685, 686, 687, 689, 690, 691,\n       692, 693, 694, 695, 696, 697, 698, 699, 700, 701, 702, 703, 704,\n       705, 707, 711, 712, 713, 714, 715, 718, 719, 721, 724, 725, 726,\n       727, 728, 729, 730, 731, 732, 733, 734, 736, 737, 740, 741, 742,\n       743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753, 755, 757,\n       760, 761, 763, 765, 767, 768, 769, 772, 773, 774, 775]),)\n(1, 544)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Isolation Forest","metadata":{"tags":[],"cell_id":"00012-4ecfed72-9f68-4218-ba9f-314752e9a02d"}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00010-be3e14e8-1c25-49ea-a641-ec776606bb71","output_cleared":false,"source_hash":"d3c1b997","execution_millis":331,"execution_start":1602523801957},"source":"from sklearn.ensemble import IsolationForest\nclf = IsolationForest(n_estimators=10, warm_start=True)\nclf.fit(train_all)  # fit 10 trees  \nclf.set_params(n_estimators=20)  # add 10 more trees  \nclf.fit(train_all)\nlabels = clf.predict(train_all)\noutliers = np.where(np.asarray(labels)==-1)\nprint(outliers)\nprint(np.shape(outliers))\n","execution_count":null,"outputs":[{"name":"stdout","text":"(array([   3,  142,  144,  153,  154,  221,  231,  338,  354,  397,  404,\n        425,  501,  634,  911,  961, 1061, 1063, 1086, 1130, 1181, 1197,\n       1207]),)\n(1, 23)\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00012-f8336c31-2f2f-4fc6-870d-51a599472907","output_cleared":false,"source_hash":"c5b4eaa8","execution_millis":139,"execution_start":1602523803806},"source":"from sklearn.ensemble import IsolationForest\nclf = IsolationForest(n_estimators=10, warm_start=True)\nclf.fit(test_all)  # fit 10 trees  \nclf.set_params(n_estimators=20)  # add 10 more trees  \nclf.fit(test_all)\nlabels = clf.predict(test_all)\noutliers = np.where(np.asarray(labels)==-1)\nprint(outliers)\nprint(np.shape(outliers))\n","execution_count":null,"outputs":[{"name":"stdout","text":"(array([309, 755]),)\n(1, 2)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Eliptic Envelope","metadata":{"tags":[],"cell_id":"00014-27ed263f-5b89-40dc-b5bc-bb5d7420daea"}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00014-173700bf-c196-4d97-a5e0-38ba3a7b8670","output_cleared":false,"source_hash":"c7b56102","execution_millis":1142595,"execution_start":1602574979846},"source":"cov = EllipticEnvelope(random_state=0, contamination=0.01)\nlabels = cov.fit_predict(x_all_norm)\noutliers = np.where(np.asarray(labels)==-1)\nprint(outliers)\nprint(np.shape(outliers))","outputs":[{"name":"stderr","text":"/opt/venv/lib/python3.7/site-packages/sklearn/covariance/_robust_covariance.py:647: UserWarning: The covariance matrix associated to your dataset is not full rank\n  warnings.warn(\"The covariance matrix associated to your dataset \"\n/opt/venv/lib/python3.7/site-packages/sklearn/covariance/_robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-20985.727148957656027 > -20999.079505866648105). You may want to try with a higher value of support_fraction (current value: 0.710).\n  RuntimeWarning)\n/opt/venv/lib/python3.7/site-packages/sklearn/covariance/_robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-20972.670761245935864 > -20982.442252451754030). You may want to try with a higher value of support_fraction (current value: 0.710).\n  RuntimeWarning)\n/opt/venv/lib/python3.7/site-packages/sklearn/covariance/_robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-20971.233180114919378 > -20988.702331796997896). You may want to try with a higher value of support_fraction (current value: 0.710).\n  RuntimeWarning)\n/opt/venv/lib/python3.7/site-packages/sklearn/covariance/_robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-20980.427971042114223 > -20990.634419138019439). You may want to try with a higher value of support_fraction (current value: 0.710).\n  RuntimeWarning)\n/opt/venv/lib/python3.7/site-packages/sklearn/covariance/_robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-20978.238108444198588 > -20991.794759793618141). You may want to try with a higher value of support_fraction (current value: 0.710).\n  RuntimeWarning)\n/opt/venv/lib/python3.7/site-packages/sklearn/covariance/_robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-20957.608917955341894 > -20997.742125562701403). You may want to try with a higher value of support_fraction (current value: 0.710).\n  RuntimeWarning)\n/opt/venv/lib/python3.7/site-packages/sklearn/covariance/_robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-20976.235871016626334 > -21005.426420474872430). You may want to try with a higher value of support_fraction (current value: 0.710).\n  RuntimeWarning)\n/opt/venv/lib/python3.7/site-packages/sklearn/covariance/_robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-20991.508474411082716 > -20995.382743110920273). You may want to try with a higher value of support_fraction (current value: 0.710).\n  RuntimeWarning)\n/opt/venv/lib/python3.7/site-packages/sklearn/covariance/_robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-20979.241170733264880 > -20984.137888262539491). You may want to try with a higher value of support_fraction (current value: 0.710).\n  RuntimeWarning)\n/opt/venv/lib/python3.7/site-packages/sklearn/covariance/_robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-20976.283077209765906 > -21004.986273491620523). You may want to try with a higher value of support_fraction (current value: 0.710).\n  RuntimeWarning)\n/opt/venv/lib/python3.7/site-packages/sklearn/covariance/_robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-21003.000367359371012 > -21003.440226479789999). You may want to try with a higher value of support_fraction (current value: 0.710).\n  RuntimeWarning)\n/opt/venv/lib/python3.7/site-packages/sklearn/covariance/_robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-20992.433533236307994 > -20994.029735578504187). You may want to try with a higher value of support_fraction (current value: 0.710).\n  RuntimeWarning)\n/opt/venv/lib/python3.7/site-packages/sklearn/covariance/_robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-20995.063412184787012 > -21002.999437205045979). You may want to try with a higher value of support_fraction (current value: 0.710).\n  RuntimeWarning)\n/opt/venv/lib/python3.7/site-packages/sklearn/covariance/_robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-20992.160590709310782 > -21012.406923474951327). You may want to try with a higher value of support_fraction (current value: 0.710).\n  RuntimeWarning)\n/opt/venv/lib/python3.7/site-packages/sklearn/covariance/_robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-20993.247384517122555 > -21001.142577468614036). You may want to try with a higher value of support_fraction (current value: 0.710).\n  RuntimeWarning)\n/opt/venv/lib/python3.7/site-packages/sklearn/covariance/_robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-20990.591984141017747 > -21015.635790183983772). You may want to try with a higher value of support_fraction (current value: 0.710).\n  RuntimeWarning)\n/opt/venv/lib/python3.7/site-packages/sklearn/covariance/_robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-20995.487071118612221 > -20998.967265724604658). You may want to try with a higher value of support_fraction (current value: 0.710).\n  RuntimeWarning)\n/opt/venv/lib/python3.7/site-packages/sklearn/covariance/_robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-20990.027963356380496 > -20990.884127305042057). You may want to try with a higher value of support_fraction (current value: 0.710).\n  RuntimeWarning)\n/opt/venv/lib/python3.7/site-packages/sklearn/covariance/_robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-20994.260122249841515 > -20996.897490721927170). You may want to try with a higher value of support_fraction (current value: 0.710).\n  RuntimeWarning)\n/opt/venv/lib/python3.7/site-packages/sklearn/covariance/_robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-20990.598660754436423 > -21001.383179649412341). You may want to try with a higher value of support_fraction (current value: 0.710).\n  RuntimeWarning)\n/opt/venv/lib/python3.7/site-packages/sklearn/covariance/_robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-20945.926926975131209 > -20976.499902964278590). You may want to try with a higher value of support_fraction (current value: 0.710).\n  RuntimeWarning)\n/opt/venv/lib/python3.7/site-packages/sklearn/covariance/_robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-20949.912866765982471 > -20975.412474805158126). You may want to try with a higher value of support_fraction (current value: 0.710).\n  RuntimeWarning)\n/opt/venv/lib/python3.7/site-packages/sklearn/covariance/_robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-20975.670482632529456 > -20976.205680875718826). You may want to try with a higher value of support_fraction (current value: 0.710).\n  RuntimeWarning)\n/opt/venv/lib/python3.7/site-packages/sklearn/covariance/_robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-20963.774399132671533 > -20981.951564755931031). You may want to try with a higher value of support_fraction (current value: 0.710).\n  RuntimeWarning)\n/opt/venv/lib/python3.7/site-packages/sklearn/covariance/_robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-20962.612127361273451 > -20999.940278188743832). You may want to try with a higher value of support_fraction (current value: 0.710).\n  RuntimeWarning)\n/opt/venv/lib/python3.7/site-packages/sklearn/covariance/_robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-20955.732963816171832 > -20982.878696363095514). You may want to try with a higher value of support_fraction (current value: 0.710).\n  RuntimeWarning)\n/opt/venv/lib/python3.7/site-packages/sklearn/covariance/_robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-20954.166407940556383 > -20968.650368592087034). You may want to try with a higher value of support_fraction (current value: 0.710).\n  RuntimeWarning)\n/opt/venv/lib/python3.7/site-packages/sklearn/covariance/_robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-21009.221383121344843 > -21014.901888155909546). You may want to try with a higher value of support_fraction (current value: 0.710).\n  RuntimeWarning)\n/opt/venv/lib/python3.7/site-packages/sklearn/covariance/_robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-21027.031514517304458 > -21034.690815972448036). You may want to try with a higher value of support_fraction (current value: 0.710).\n  RuntimeWarning)\n/opt/venv/lib/python3.7/site-packages/sklearn/covariance/_robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-21034.014184650153766 > -21040.061861962210969). You may want to try with a higher value of support_fraction (current value: 0.710).\n  RuntimeWarning)\n/opt/venv/lib/python3.7/site-packages/sklearn/covariance/_robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-21023.757784750145220 > -21028.914184012050100). You may want to try with a higher value of support_fraction (current value: 0.710).\n  RuntimeWarning)\n/opt/venv/lib/python3.7/site-packages/sklearn/covariance/_robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-21023.279333747668716 > -21042.895300147461967). You may want to try with a higher value of support_fraction (current value: 0.710).\n  RuntimeWarning)\n/opt/venv/lib/python3.7/site-packages/sklearn/covariance/_robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-20993.944630947979022 > -21000.129271424862964). You may want to try with a higher value of support_fraction (current value: 0.710).\n  RuntimeWarning)\n/opt/venv/lib/python3.7/site-packages/sklearn/covariance/_robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-20999.122156649402314 > -20999.739652875607135). You may want to try with a higher value of support_fraction (current value: 0.710).\n  RuntimeWarning)\n/opt/venv/lib/python3.7/site-packages/sklearn/covariance/_robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-20983.943553001336113 > -21002.144549685490347). You may want to try with a higher value of support_fraction (current value: 0.710).\n  RuntimeWarning)\n/opt/venv/lib/python3.7/site-packages/sklearn/covariance/_robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-20970.230484581312339 > -20996.236342002262973). You may want to try with a higher value of support_fraction (current value: 0.710).\n  RuntimeWarning)\n/opt/venv/lib/python3.7/site-packages/sklearn/covariance/_robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-20981.729795068742533 > -20983.258504554527462). You may want to try with a higher value of support_fraction (current value: 0.710).\n  RuntimeWarning)\n/opt/venv/lib/python3.7/site-packages/sklearn/covariance/_robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-20965.433688216719020 > -20982.405879927049682). You may want to try with a higher value of support_fraction (current value: 0.710).\n  RuntimeWarning)\n/opt/venv/lib/python3.7/site-packages/sklearn/covariance/_robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-20977.734464618806669 > -21001.733038920210674). You may want to try with a higher value of support_fraction (current value: 0.710).\n  RuntimeWarning)\n/opt/venv/lib/python3.7/site-packages/sklearn/covariance/_robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-20974.240119368176238 > -20989.075117846776266). You may want to try with a higher value of support_fraction (current value: 0.710).\n  RuntimeWarning)\n/opt/venv/lib/python3.7/site-packages/sklearn/covariance/_robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-20983.749262280533003 > -20985.713633203577047). You may want to try with a higher value of support_fraction (current value: 0.710).\n  RuntimeWarning)\n/opt/venv/lib/python3.7/site-packages/sklearn/covariance/_robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-20973.016815208218759 > -20976.004424731323525). You may want to try with a higher value of support_fraction (current value: 0.710).\n  RuntimeWarning)\n(array([  28,  313,  352,  426,  684,  710,  732, 1217, 1321, 1354, 1384,\n       1442, 1456, 1521, 1522, 1552, 1612, 1784, 1837, 1945]),)\n(1, 20)\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00007-99457c02-c67c-439e-b212-99b20642070c","output_cleared":false,"source_hash":"9dd383a1","execution_millis":43,"execution_start":1602576122448},"source":"# check for critical value:\nfrom scipy.stats import chi2\nchi2.ppf((1-0.05), df=832)","outputs":[{"output_type":"execute_result","execution_count":12,"data":{"text/plain":"900.214769720817"},"metadata":{}}],"execution_count":null},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00008-3a357857-d807-44fb-902f-3927e383821d","output_cleared":false,"source_hash":"1f6fd16e","execution_start":1602237945055,"execution_millis":43},"source":"def mahalanobis(x=None, data=None, cov=None):\n\n    x_mu = x - np.mean(data)\n    if not cov:\n        cov = np.cov(data.values.T)\n    inv_covmat = np.linalg.inv(cov)\n    left = np.dot(x_mu, inv_covmat)\n    mahal = np.dot(left, x_mu.T)\n    return mahal.diagonal()","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00010-71a28bf0-6965-45e3-8548-4d7c5fb7f249","output_cleared":false,"source_hash":"cb3e3f79","execution_millis":143,"execution_start":1602240653678},"source":"# there are 1175 correlations > 0.9 (absolute) (diag has to be taken away: 171 single correlations! rro\nfor i in range(0,832):\n    print(i)\n    C_mat = train_all[i, 'age'].corr()\nprint(C_mat)\n#high_corr_mask = (abs(C_mat)>0.9)\n#print(high_corr_mask.sum().sum())\n","execution_count":null,"outputs":[{"name":"stdout","text":"0\n","output_type":"stream"},{"output_type":"error","ename":"KeyError","evalue":"(0, 'age')","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m/opt/venv/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2645\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2646\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2647\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: (0, 'age')","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m<ipython-input-91-5257278e96c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m832\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mC_mat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_all\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'age'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mC_mat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mhigh_corr_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mC_mat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/venv/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2798\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2799\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2800\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2801\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2802\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/venv/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2646\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2647\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2648\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2649\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2650\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: (0, 'age')"]}]}],"nbformat":4,"nbformat_minor":2,"metadata":{"orig_nbformat":2,"deepnote_notebook_id":"e326d53e-92d1-495e-81fe-8d0bb65c88f8","deepnote_execution_queue":[]}}